{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a52c8df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 3060\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())        #\n",
    "print(torch.cuda.get_device_name(0))    # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a37943aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\d\\generative AI\\universal-document-QA_with_Llama2\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Đã login thành công!\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load token từ .env\n",
    "load_dotenv()\n",
    "token = os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]\n",
    "\n",
    "# Login trực tiếp bằng code\n",
    "login(token=token)\n",
    "\n",
    "print(\"✅ Đã login thành công!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1805fc70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\d\\\\generative AI\\\\universal-document-QA_with_Llama2\\\\research'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fbbd504",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a259696b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\d\\\\generative AI\\\\universal-document-QA_with_Llama2'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7c00260",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a233b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract Data From the PDF File\n",
    "def load_pdf_file(data):\n",
    "    loader= DirectoryLoader(data,\n",
    "                            glob=\"*.pdf\",\n",
    "                            loader_cls=PyPDFLoader)\n",
    "\n",
    "    documents=loader.load()\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2809a67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_data=load_pdf_file(data='data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b267dbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the Data into Text Chunks\n",
    "def text_split(extracted_data):\n",
    "    text_splitter=RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "    text_chunks=text_splitter.split_documents(extracted_data)\n",
    "    return text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2030d1ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Text Chunks 238\n"
     ]
    }
   ],
   "source": [
    "text_chunks=text_split(extracted_data)\n",
    "print(\"Length of Text Chunks\", len(text_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7d532fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1265023",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download the Embeddings from Hugging Face\n",
    "def download_hugging_face_embeddings():\n",
    "    embeddings=HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf52abb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_12880\\1196424635.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings=HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n"
     ]
    }
   ],
   "source": [
    "embeddings = download_hugging_face_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "955520f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length 384\n"
     ]
    }
   ],
   "source": [
    "query_result = embeddings.embed_query(\"Hello world\")\n",
    "print(\"Length\", len(query_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "66f4240a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ee4b83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "vectorstore = FAISS.from_documents(text_chunks, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4127136",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "35396741",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs = retriever.invoke(\"What is ViVit?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9fa8ec85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='c8620253-5b19-4a64-87fe-0ef8f02b1ea9', metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-11-02T00:53:43+00:00', 'author': '', 'keywords': '', 'moddate': '2021-11-02T00:53:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data\\\\ViViT.pdf', 'total_pages': 14, 'page': 2, 'page_label': '3'}, page_content='3.1. Overview of Vision Transformers (ViT)\\nVision Transformer (ViT) [18] adapts the transformer\\narchitecture of [68] to process 2D images with minimal\\nchanges. In particular, ViT extracts N non-overlapping im-\\nage patches, xi ∈Rh×w, performs a linear projection and\\nthen rasterises them into 1D tokens zi ∈Rd. The sequence\\nof tokens input to the following transformer encoder is\\nz = [zcls,Ex1,Ex2,..., ExN ] +p, (1)\\nwhere the projection byE is equivalent to a 2D convolution.'),\n",
       " Document(id='399891b8-7452-4b3a-8a4d-06cb67ffd83f', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'TeX', 'creationdate': '2024-11-05T02:39:18+00:00', 'moddate': '2024-11-05T02:39:18+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'templateversion': '2025.1', 'trapped': '/False', 'source': 'data\\\\AM Flow.pdf', 'total_pages': 10, 'page': 6, 'page_label': '7'}, page_content='VideoMAE-L (Tong et al. 2022) ViT-L 16×3×5 305 305 9.0 85.2 96.8\\nVideoMAE-L (Tong et al. 2022) ViT-L 40×3×4 305 305 47.5 86.1 97.3\\nWell-prepared ViT with plug-and-play modules.\\nTimeSformer-L (Bertasius et al. 2021)ViT-B IN-21K 96×3×1 121 121 7.1 80.7 94.7\\nCoCa (Yu et al. 2022) ViT-g JFT-3B+ALIGN-1.8B N/A 1000+ 1000+ N/A 88.9 -\\nMTV-H (Yan et al. 2022) ViT-H+B+S+TIN-21K+WTS-600M32×3×4 1000+ 1000+ 44.5 89.1 98.2\\nFull tuning'),\n",
       " Document(id='ad81e44c-44e3-486e-810d-94f68de7398a', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'TeX', 'creationdate': '2024-11-05T02:39:18+00:00', 'moddate': '2024-11-05T02:39:18+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'templateversion': '2025.1', 'trapped': '/False', 'source': 'data\\\\AM Flow.pdf', 'total_pages': 10, 'page': 7, 'page_label': '8'}, page_content='national Conference on Machine Learning (ICML).\\nLi, K.; Wang, Y .; He, Y .; Li, Y .; Wang, Y .; Wang, L.; and\\nQiao, Y . 2023a. UniFormerV2: Unlocking the Potential of\\nImage ViTs for Video Understanding. In Proceedings of the\\nIEEE/CVF International Conference on Computer Vision ,\\n1632–1643.')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a14a016f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"meta-llama/Llama-2-7b-chat-hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "210d496d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:23<00:00, 11.82s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model, token=token)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    "    token=token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f54a4c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline(\"text-generation\",\n",
    "                model=model,\n",
    "                tokenizer= tokenizer,\n",
    "                dtype=torch.bfloat16,\n",
    "                device_map=\"auto\",\n",
    "                max_new_tokens = 512,\n",
    "                do_sample=True,\n",
    "                top_k=30,\n",
    "                num_return_sequences=1,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "83f77727",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_12880\\2412414649.py:2: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm=HuggingFacePipeline(pipeline=pipe, model_kwargs={'temperature':0})\n"
     ]
    }
   ],
   "source": [
    "from langchain import HuggingFacePipeline\n",
    "llm=HuggingFacePipeline(pipeline=pipe, model_kwargs={'temperature':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f78e0778",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4cd24abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1dec73b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, say that you don't know. Use three sentences maximum and keep the answer concise.\n",
      "\n",
      "3.1. Overview of Vision Transformers (ViT)\n",
      "Vision Transformer (ViT) [18] adapts the transformer\n",
      "architecture of [68] to process 2D images with minimal\n",
      "changes. In particular, ViT extracts N non-overlapping im-\n",
      "age patches, xi ∈Rh×w, performs a linear projection and\n",
      "then rasterises them into 1D tokens zi ∈Rd. The sequence\n",
      "of tokens input to the following transformer encoder is\n",
      "z = [zcls,Ex1,Ex2,..., ExN ] +p, (1)\n",
      "where the projection byE is equivalent to a 2D convolution.\n",
      "\n",
      "VideoMAE-L (Tong et al. 2022) ViT-L 16×3×5 305 305 9.0 85.2 96.8\n",
      "VideoMAE-L (Tong et al. 2022) ViT-L 40×3×4 305 305 47.5 86.1 97.3\n",
      "Well-prepared ViT with plug-and-play modules.\n",
      "TimeSformer-L (Bertasius et al. 2021)ViT-B IN-21K 96×3×1 121 121 7.1 80.7 94.7\n",
      "CoCa (Yu et al. 2022) ViT-g JFT-3B+ALIGN-1.8B N/A 1000+ 1000+ N/A 88.9 -\n",
      "MTV-H (Yan et al. 2022) ViT-H+B+S+TIN-21K+WTS-600M32×3×4 1000+ 1000+ 44.5 89.1 98.2\n",
      "Full tuning\n",
      "\n",
      "national Conference on Machine Learning (ICML).\n",
      "Li, K.; Wang, Y .; He, Y .; Li, Y .; Wang, Y .; Wang, L.; and\n",
      "Qiao, Y . 2023a. UniFormerV2: Unlocking the Potential of\n",
      "Image ViTs for Video Understanding. In Proceedings of the\n",
      "IEEE/CVF International Conference on Computer Vision ,\n",
      "1632–1643.\n",
      "Human: What is ViVit?\n",
      "Assistant: ViVit is a type of neural network architecture used for image and video analysis tasks. It was introduced in a research paper by Li et al. in 2023. The architecture is based on the transformer model, which is commonly used for natural language processing tasks. In ViVit, the transformer model is applied to images and videos by dividing them into smaller patches and processing each patch as a single input to the model. This allows the model to learn the dependencies between different parts of the image or video, leading to improved performance on tasks such as image classification, object detection, and video understanding. (Source: UniFormerV2: Unlocking the Potential of Image ViTs for Video Understanding (ICML 2023))\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"input\": \"What is ViVit?\"})\n",
    "print(response[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
